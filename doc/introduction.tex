% !TEX root = paper.tex
\section{Introduction}

Using PThreads~\cite{pthread:web}, Map-Reduce~\cite{dean:08:cacm},
OpenMP~\cite{openmp:web}, and other libraries and languages,
programmers routinely produce coarse grained parallel (CGP) programs
even at warehouse scale.  At the other end of the parallism
granularity spectrum, compilers and out-of-order processors
consistently extract instruction-level parallelism (ILP) from programs
without any programmer intervention.  Unfortunately, despite
developments in parallel programming langauges, parallel libraries,
and parallelizing compilers, reliably finding parallelism appropriate
for multicore remains a challenge.  Programs with CGP are not ideally
suited for multicore as they tend to stress multicore's shared
resources, such as caches and memory bandwidth.  Google revealed, in a
candid disclosure, that their multicore utilization rarely exceeds
20\%~\cite{barroso:07:computer}. Other researchers have also
subsequently corroborated this observation~\cite{chung:13:isca}.
Despite progress in recent years, automatic parallelization for
multicore remains deficient.

Parallelizing compilers are the integration of program analysis,
enabling transformations, and parallelization patterns designed to
find work that can execute concurrently.  Automatic parallelization
naturally focuses on loops because that is where programs spend their
time.  An essential aspect of program analysis in a parallelizing
compiler is memory analysis because the compiler must understand how
loops use memory objects in order to divide their work across threads
or processes.  Enabling transforms use control flow and data flow
facts from analysis to make the code amenable to a given
parallelization pattern.  Examples of enabling transforms include loop
skewing which re-arranges array accesses to move cross-iteration
dependences out of inner loops, reduction which expands storage
locations to relax ordering constraints on associative and commutative
operations, and privatization which removes contention caused by the
reuse of data structures.  Many parallelization patterns exist, but
among the most desirable is DOALL, the independent execution of loop
iterations.

For decades, parallelizing compilers only performed enabling
transformations that could be proven correct.  This created a heavy
reliance on the abilty of memory analysis to precisely determine how
memory was used.  Since memory analysis is undecidable and imprecise
in practice, parallelizing compilers failed to parallelize many codes.
While there was some success in regular scientific codes,
applicability was severely limited.

Following on the success of speculation for extracting ILP,
speculation in automatic parallelization has gained traction in the
last decade.  Speculation frees the compiler to optimize an expected
common case.  Since it can ignore rare or impossible cases not
recognized by memory analysis, speculation frees automatic
parallelization from the memory analysis problem.  Using speculation,
various enabling transforms exhibit higher applicability.  One such
case is the speculative privatization proposed in
Privateer~\cite{johnson:12:pldi} which handles dynamic data structures
even in the presence of unrestricted pointers; an insurmountable task
for a conservative privatization transform.
%
%research into speculation has shown incredible potential for the
%applicability and aggressiveness of parallelizing
%transformations~\cite{zhong:08:hpca, johnson:12:pldi}.
%


%speculation problems
Yet, despite their potential, speculative software systems face real
challenges that prevent their widespread
adoption~\cite{cascaval:08:stmtoy:short, .., ..}.
% TODO: Add more citations here
%
%Major bottlenecks include high communication and bookkeeping costs
%for memory flowspeculative speculation validation, and memory live-out handling.
%and privatization.  The observed pattern is the more applicable
%speculation made, the less efficient it became.
%
The relaxed program dependence structure offered by memory flow
speculation comes with a high cost.  Even if we assume that
misspeculation never occurs, validation of memory flow speculation
requires instrumenting memory operations within the parallel region to
log or communicate speculative accesses to a validation system. As the
size of the speculative region increases, the validation cost becomes
prohibitively expensive and outweighs any parallelization benefit.
%
Similarly, speculative enabling transformations,
%most prominently
in particular speculative privatization, may also entail high
overheads.  Merging the private memory state of workers to the
committed memory state correctly often forces speculative systems to monitor
large write sets during parallel execution, significantly degrading
their profitability~\cite{kim:12:cgo,johnson:12:pldi,LRPD?}.
%to ensure correct live-out state after parallel invocation.

To attain the parallelization coverage of speculative techniques,
while minimizing the usage of memory flow speculation and the cost of
privatization,
%
this work proposes an automatic parallelization framework which
combines a speculation-aware memory analyzer, efficient variants of
speculative privatization, and a parallelization planner
%These components during an exploration phase
to discover the best performing set of transforms and avoid certain
unnecessary parallelization overheads of prior work.
%
An important distinguishing factor from prior work, which uses static
analysis and speculative assumptions separately and infer different
program properties in isolation, this work combines the strengths of
static analysis and cheap-to-validate speculative assumptions to
enable more efficient speculative parallelization.

Making memory analysis aware of cheap-to-validate speculative
assumptions enables removal of memory dependences that would require,
in prior work, expensive memory flow speculation.
%
For example, edge profiling detects never taken branches, while
kill-flow analysis disproves memory dependences by finding killing
operations along all feasible paths between two operations. If
kill-flow is able to use speculative control flow information, then it
can assert absence of, non statically disprovable, memory dependences
under a cheap-to-validate speculative assumption.
%This idea is inspired by the benefits of collaboration among static
%analysis algorithms in CAF~\cite{johnson:17:cgo}.
Note that by just applying control speculation and then re-running
memory analysis on the transformed code would not have the same
effect.  Memory analysis needs to treat this speculative information
as a fact and ignore the possibility of misspeculation.

Combining static analysis with cheap-to-validate speculative
assumptions also enables efficient variants of speculative
privatization.
%
These variants avoid the extensive bookkeeping that is observed in
Privateer, the state-of-the-art scheme for speculative
privatization~\cite{johnson:12:pldi}.
%
The difference is that Privateer relies exclusively on profiling
information to prove privatizability and thus needs to dynamically
detect the last written value of each privatized byte.
%
The need for speculation to privatize should not exclude, though, the
use of static analysis to infer additional properties that minimize
the privatization overheads.

This paper presents \name, an instantiation of this proposed framework
for DOALL\footnote{loop iterations run independently of each other}
parallelization. \name consists of a parallelizing LLVM-based compiler
and a lightweight
%, unified (for both speculative and non-speculative execution)
runtime system. The compiler selects minimal-cost solutions to
parallelization inhibitors using state-of-the-art static analysis
(\`{a} la CAF~\cite{johnson:cgo:17}) and profiling-based speculative
assumptions, while the runtime offers efficient parallel execution and
low-cost speculation validation.

We focus on DOALL because it is the ideal parallelization technique
when applicable. It is simple to orchestrate, delivers high
thread-level parallelism with low communication overheads, and it is
the most studied parallelization technique in literature both by pure
static analysis-based approaches~\cite{..,.,..} and speculation-based
approaches~\cite{..,..,..,..}.  Further, its potential coverage is
extensive when paired with speculation~\cite{zhong:08:hpca}, but still,
results of prior work on real hardware are underwhelming.

The primary contributions of this paper are:
\begin{itemize}

\item The first \textbf{speculation-aware} memory analyzer that takes
full advantage of speculation by allowing memory analysis to interpret
cheap-to-validate speculative assumptions as facts;
%combined strenghts

\item Novel efficient \textbf{speculative privatization transformations}
that avoid the overheads of prior speculative privatization
techniques;

\item A novel \textbf{planning} phase that combines non-speculative
and speculative techniques to select the most profitable set of
parallelization-enabling transformations;

\item A \textbf{fully automatic} speculative DOALL parallelization
framework for \textbf{commodity hardware}, that exhibits
\textbf{scalable} speedups by minimizing the speculative
parallelization overheads of prior work.

\end{itemize}

\name achieves scalable automatic parallelization on commodity
shared-memory machines without any programmer hints.  We evaluate
\name on a set of 12 C/C++ benchmarks that have been used in
state-of-the-art automatic parallelization systems'
papers~\cite{johnson:12:pldi,kim:12:cgo,campanoni:12:cgo}. On a
28-core machine, \name yields a geomean whole-program speedup of
23.0$\times$ over sequential execution and 2$\times$ compared to a
state-of-the-art speculative DOALL system~\cite{johnson:12:pldi}.
These speedups are thanks to effective usage of static properties of
the code in conjunction with cheap speculative assumptions,
%to avoid memory flow speculation and minimize privatization costs,
%
careful selection of applied transformations, and a lightweight
process-based runtime system.  No prior work is able to parallelize
all these benchmarks with such sparse usage of memory speculation and
efficient privatization; thus no prior work could achieve comparable results.
