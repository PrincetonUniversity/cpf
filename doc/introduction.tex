\section{Introduction}

% SIMONE: overall = I like the flow, but I think it would be nice to (i) anticipate the novelty of this paper and (ii) highlight much more it. For example, early in the introduction we can say that today's approaches are either speculative without the use of static analysis or conservative without the use of speculative techniqeus. Both approaches have advantages and disadvantages. We are the first ones to put them together to leverage their advantages and avoid their weaknesses.

% AutoPar is important.
%Coincident with the introduction of multicore, the processor industry has
%fallen well short of the decades-old single-threaded performance growth
%trend. The burden of extracting sufficient multicore-appropriate parallelism
%to overcome this sequential performance deficit has fallen on programmers
%and compilers. Yet, despite many new tools, languages, and libraries
%designed for multicore, the difficulty of parallelism extraction keeps
%multicore grossly underutilized.

%Parallelization is necessary

Multicore architectures are ubiquitous in systems ranging from mobile to
``warehouse scale" systems. Thus, parallelism is more important than ever to
harvest more performance from current hardware.  This burden of extracting
sufficient parallelism has fallen on programmers and compilers. Yet, despite
many new tools, languages, and libraries designed for multicore, multicore
remains a grossly underutilized technology.


%Programmers?

%There are many ways programmers can manually realize parallelism.  Common
%approaches, such as PThreads~\cite{pthread:web},
%Map-Reduce~\cite{dean:08:cacm}, and OpenMP~\cite{openmp:web}, generally help
%programmers extract coarse grained parallelism (CGP) from hot loops in
%applications.
Manually expressing parallelism is tedious and error-prone. The programmers that
do attempt parallelism typically rely on popular approaches (e.g.,
PThreads~\cite{pthread:web}, Map-Reduce~\cite{dean:08:cacm}, and
OpenMP~\cite{openmp:web}) that focus on extracting coarse grained parallelism
(CGP) from hot loops in applications.
%
%CGP can be extremely beneficial in cluster and warehouse-scale systems, but has
%limitations on multicore.
Programs with CGP are not ideally suited though for multicore as they tend to
stress multicore's shared resources, such as caches and memory bandwidth.
Google revealed, in a candid disclosure, that their multicore utilization rarely
exceeds 20\%~\cite{barroso:07:computer}. Other researchers have also
subsequently corroborated this observation~\cite{chung:13:isca}.
%
To make matters worse, for programs with complex control and data flow, even CGP
extraction is often extremely difficult.
%
Asking programmers to address this underutilization with finer-grained
parallelism extraction would result in enormous increase in programmers' effort.
%

%can be very difficult, and most such programs remain without a CGP extracted
%version.
%
% SIMONE: should we say, instead, that these programs either don't have CGP at
% all (e.g., GCC) or what they have is enough for a few cores only and it is a
% weak-scaling rather than strong scaling (e.g., Chrome has CGP where one tab is
% assigned to one thread; the more tabs, the more threads, the more CGP => weak
% scaling)?

%autopar is very useful
Automatic parallelization is an attractive approach to make efficient use of
multicore systems, yielding performance improvement even for already parallelized
applications (e.g, reduced maximum latency on each parallel task of a
CGP-parallelized application). Unfortunately, even after 15 years of multicore,
automatic parallelization is still mostly limited to scientific and
data-intensive applications.
%the routine extraction of TLP remains elusive.

%analyis limitations
Major impediments to automatic parallelization are the limitations of static
analysis. Static dependence analysis (including alias analysis and control flow
analysis) are undecidable and difficult in practice, so compilers rely upon
imprecise static analysis that limits parallelizing compilers applicability to
regular code patterns (e.g., scientific applications) and small loops, or forces
expensive inter-thread communication that severely hinders scalability.
%TODO: ADD citations to non-spec parallelizing compilers.  is there a reference
%that shows that HELIX byitself does not scale to more cores?  cite PS-DSWP and
%doall non-spec parallelization (e.g., SUIF)
%
%Static analysis alone is simply too imprecise to enable aggressive
%parallelization.
In addition to being conservative, static analysis describes program facts that
are true independently of the input, resulting in missed opportunities for
parallelizing compilers.
%the dependence patterns of real applications are frequently driven by the
%program input, or experience a phase change during program execution
Current static parallelizing compilers~\cite{HELIX, PS-DSWP, SUIF, non-SPEC
doall ...} when applicable can deliver good predictable speedups, but they all
suffer from the aforementioned limitations, and consequently fail to enable
aggressive and scalable parallelization for most general-purpose applications.

%speculation necessary
Speculation allows the compiler to overcome static analysis' constraints,
optimize for an expected
%/common
case, and ignore rare or impossible dependences.  Research into speculation has
shown incredible potential for the applicability and aggressiveness of
parallelizing transformations~\cite{Uncovering_malhke, privateer}.
%Speculative parallelization exploits simplifying assumptions about the input
%code to deliver increased parallel performance.
%
%speculation problems
Yet, speculative runtime systems face real challenges that prevent their
widespread adoption~\cite{cascaval:08:stmtoy:short, ..}. Even if we assume that
misspeculation never occurs, validating speculative assumptions during a
speculative region may heavily degrade performance.
%
Most speculative systems perform this validation by instrumenting operations
within the parallel region to log or communicate speculative events to a
validation system.  These validation checks add to parallel region latency in
the common case, not only during recovery.

Many proposals speculate low-level properties of the code.
%
Such speculative systems work well when assisted by hardware
extensions~\cite{TLS papers...}, but yield small speedups, if any, when running
on commodity hardware.
%TODO: other software-spec apart from STM, more recent ones ??
For example, Software Transactional Memories (STMs)~\cite{stmlite, ..} present
an abstraction that facilitates speculating the independence of memory
transactions (i.e., memory speculation). Validation requires logging or
communication for most of the memory accesses within each transaction and
becomes prohibitively expensive as the size of transaction grows.
%
%This reduces to comparing the read and write sets of adjacent transactions.  As
%transactions grow, the number of memory operations within that transaction can
%become prohibitively large. Further, instrumenting every memory operation with
%the transaction to log or communicate every access---approximately one tenth of
%all dynamic instructions---leads to an excessive overhead, even in the absence
%of transaction rollbacks.

%privateer
Other speculative systems attempt to avoid the excessive memory speculation of
generic transactional systems by speculating higher-level properties whose
validation does not require logging or communication.
%
A prominent work of this class is Privateer~\cite{johnson:12:pldi:short}, a
state-of-the-art Spec-DOALL parallelization system, that exhibits more scalable
speedups compared to prior work.
%
Privateer partitions memory objects into several categories/families according
to observed access patterns via profiling.  Speculating that certain
individual memory access pairs are independent is avoided by just speculating
separation of the families and some other simple properties.  However, detection
of privatizable memory objects relies solely on memory and control profiling.
Therefore, validation for a substantial set of memory objects still requires
expensive logging and checks at every memory access, similarly to STM systems.
% and occassional communication among workers
%cheap spec for short-lived or read-only has also been discussed in other spec
%systems such as cluster-doall and corD
%

The common observed pattern that limits the gains of speculative systems is
excessive usage of expensive-to-validate speculation and insufficient, if any,
leverage of statically provable properties of the code.

To attain the coverage of speculative techniques while minimizing usage of
expensive speculation,
%expensive memory speculation.
we propose a unified parallelization framework that combines both static and
speculative techniques.
%
This combination is not as simple as using a static and a speculative compiler
in a sequence~\cite{clusterDOALL,...}; this simple combination would be
undesirable as the limitations of each compiler are still present.
%
We suggest that the selected parallelization plan contains, if necessary, both
speculative and conservative parallelization enabling transformations.
%
In terms of ordering, an intuitive approach would be to consider all the
conservative tranformations before the speculative ones. We disprove this
intuition for certain cases. We propose instead that all techniques are
considered together and that the more efficient option for each parallelization
obstacle is chosen.
%
Our proposed framework also includes enhanced program analysis.  In prior work,
static analysis and speculative assumptions are considered separetely, each
disproving different memory dependences. Instead, we propose that static analyis
works synergistically with speculative assumptions to infer program properties
that would not be discovered in isolation.
%similarly to how collaboration of static analysis passes works instead of
%additive, multiplicative effect work synergistically together ctrl
%spec+killflow, value_pred+alias analysis


This paper presents LSD, an instantiation of this proposed framework for DOALL
parallelization. We focus on DOALL parallelization, as a first step, since it is
a simple, yet very effective~\cite{Uncovering Mahlke paper}, and well-studied,
especially in terms of speculation, parallelization technique.
%
LSD consists of a state-of-the-art static analysis (CAF~\cite{..}) that maximizes
statically disproven loop-carried dependences, a set of profilers that generate
speculative assumptions, a parallelizing LLVM-based compiler that selects
minimal-cost solutions to parallelization inhibitors from a wide range of
enabling transformations, and a lightweight, unified (for both speculative and
non-speculative) runtime system for efficient parallel execution and low-cost
speculation validation.
%

LSD achieves scalable automatic parallelization on commodity shared-memory
machines without any programmer hints.  We evaluate LSD on a set of 12 C/C++
programs that appear on prior DOALL parallelization systems' papers. On a
28-core machine, LSD yield a geomean whole-program speedup of Xx over sequential
execution and Yx compared to Privateer.
%better or as good as prior work
These speedups are thanks to effective usage of static properties of the code in
conjunction with cheap speculative assumptions, careful selection of applied
transformations and a lightweight process-based runtime.  No prior work is able
to parallelize all these benchmarks without any usage of memory speculation,
thus no prior work could achieve comparable results.
