% !TEX root = paper.tex
\section{Introduction}
% SIMONE: title = I would change it to emphasize the main novelty of this work, which is merging static analysis and speculation to reach more than the sum of the parts

% SIMONE: version 1 of Section 1: overall feedback = I think it improved since version 0. However, I think it is too long, but there are opportunities to shrink it considerably. This will bring the description of the novelty of the paper much sooner. For example, consider the paragraph about privateer. It doesn't look like we need that argument for introducing the novelty of this paper. Hence, that paragraph can go to Section 2 (Background and Motivation). Also, I would emphasize much more the fact that merging static analysis with speculation results to more than the sum of the parts. Then, I would describe the intuition behind this claim and, finally, the results of this fact (e.g., no prior work is able to parallelize all benchamrks we do parallelize in this paper).

% SIMONE: version 0 of Section 1: overall feedback = I like the flow, but I think it would be nice to (i) anticipate the novelty of this paper and (ii) highlight much more it. For example, early in the introduction we can say that today's approaches are either speculative without the use of static analysis or conservative without the use of speculative techniqeus. Both approaches have advantages and disadvantages. We are the first ones to put them together to leverage their advantages and avoid their weaknesses.

% AutoPar is important.
%Coincident with the introduction of multicore, the processor industry has
%fallen well short of the decades-old single-threaded performance growth
%trend. The burden of extracting sufficient multicore-appropriate parallelism
%to overcome this sequential performance deficit has fallen on programmers
%and compilers. Yet, despite many new tools, languages, and libraries
%designed for multicore, the difficulty of parallelism extraction keeps
%multicore grossly underutilized.

%Parallelization is necessary
Multicore architectures are ubiquitous in systems ranging from mobile to
``warehouse scale'' systems. Thus, parallelism is more important than ever to
harvest more performance from current hardware.  This burden of extracting
sufficient parallelism has fallen on programmers and compilers. Yet, despite
many new tools, languages, and libraries designed for multicore, multicore
remains a grossly underutilized technology.


%Programmers?

%There are many ways programmers can manually realize parallelism.  Common
%approaches, such as PThreads~\cite{pthread:web},
%Map-Reduce~\cite{dean:08:cacm}, and OpenMP~\cite{openmp:web}, generally help
%programmers extract coarse grained parallelism (CGP) from hot loops in
%applications.
Manually expressing parallelism is tedious and error-prone. The programmers that
do attempt parallelism typically rely on popular approaches (e.g.,
PThreads~\cite{pthread:web}, OpenMP~\cite{openmp:web}) that focus on extracting
coarse grained parallelism from a few hot loops with simple control and data
flow. Even if manual parallelization is successfully applied,
%a program is manually parallelized,
it is still an unappealing option due to the high maintenance cost of parallel
code compared to sequential (five times more expensive according to
Google~\cite{google_cite_simone}).
%
%CGP can be extremely beneficial in cluster and warehouse-scale systems, but has
%limitations on multicore.  Programs with CGP are not ideally suited though for
%multicore as they tend to stress multicore's shared resources, such as caches
%and memory bandwidth.  Google revealed, in a candid disclosure, that their
%multicore utilization rarely exceeds 20\%~\cite{barroso:07:computer}. Other
%researchers have also subsequently corroborated this
%observation~\cite{chung:13:isca}.
%
%To make matters worse, for programs with complex control and data flow, even
%CGP extraction is often extremely difficult.
%
%Asking programmers to address this underutilization with finer-grained
%parallelism extraction would result in enormous increase in programmers'
%effort.
%
%can be very difficult, and most such programs remain without a CGP extracted
%version.
%
% SIMONE: should we say, instead, that these programs either don't have CGP at
% all (e.g., GCC) or what they have is enough for a few cores only and it is a
% weak-scaling rather than strong scaling (e.g., Chrome has CGP where one tab is
% assigned to one thread; the more tabs, the more threads, the more CGP => weak
% scaling)?

%autopar is very useful
Automatic parallelization is an attractive approach to make efficient use of
multicore systems.
%, yielding performance improvement even for already parallelized applications
%(e.g, reduced maximum latency on each parallel task of a CGP-parallelized
%application).
Unfortunately, even after 15 years of multicore, automatic parallelization is
still mostly limited to scientific and data-intensive applications.
%the routine extraction of TLP remains elusive.

%analyis limitations
Major impediments to automatic parallelization are the limitations of static
analysis, especially for general-purpose C/C++ programs. Static dependence
analysis (including alias analysis and control flow analysis) is undecidable and
difficult in practice, so compilers rely upon imprecise static analysis that
limits parallelizing compilers applicability to regular code patterns (e.g.,
scientific applications) and small loops, or forces expensive inter-thread
communication that severely hinders scalability.
%
%Static analysis alone is simply too imprecise to enable aggressive
%parallelization.  In addition to being conservative, static analysis describes
%program facts that are true independently of the input, resulting in missed
%opportunities for parallelizing compilers.
%

%Efforts to augment static analysis with low-cost run-time analysis are limited
%to simple cases where a predicate can be extracted outside the loop of
%interest~\cite{hybrid_analysis, suif:94:stanford, polaris}.
%
%%TODO: check if it is only affine loops for hybrid. that's the case for suif and
%%polaris
%
%%the dependence patterns of real applications are frequently driven by the
%%program input, or experience a phase change during program execution
%Overall, current analysis-based parallelizing
%compilers~\cite{campanoni:2012:iscgo, raman:2008:iscgo, suif:94:stanford,
%polaris, sensitivity} when applicable can deliver good predictable speedups, but
%they all suffer from the aforementioned limitations, and consequently fail to
%enable aggressive and scalable parallelization for most general-purpose
%applications.

%speculation necessary
Speculation allows the compiler to overcome static analysis' constraints,
optimize for an expected
%/common
case, and ignore rare or impossible dependences.  Research into speculation has
shown incredible potential for the applicability and aggressiveness of
parallelizing transformations~\cite{zhong:08:hpca, johnson:12:pldi:short}.
%Speculative parallelization exploits simplifying assumptions about the input
%code to deliver increased parallel performance.
%
%
%speculation problems
Yet, software speculative systems face real challenges that prevent their
widespread adoption~\cite{cascaval:08:stmtoy:short, .., ..}.
% TODO: Add more citations here
Major bottlenecks include high communication and bookkeeping costs for memory
flow speculation validation
%and memory live-out handling.
and privatization.

Even if we assume that misspeculation never occurs, validating speculative
assumptions adds to the parallel region's latency in the common case, not only
during recovery, and may heavily degrade performance.
%
In particular, validation of memory flow speculation requires instrumenting
memory operations within the parallel region to log or communicate speculative
accesses to a validation system. As the size of the speculative region
increases, the validation cost becomes prohibitively expensive and outweighs any
parallelization benefit.
%read/write sets grow
%
Many speculative parallelizing compilers avoid checks on data flows that can be
disproven by static analysis~\cite{stmlite, LRPD,..}, and some suggest
cheaper-to-validate speculative assumptions to remove a few additional
checks~\cite{privateer, ..}.
%speculating higher-level properties whose validation does not require logging
%or communication.
%
Still, current software speculative systems do not sufficiently leverage static
and easy-to-validate assumptions; hence they fail to avoid excessive use of
memory flow speculation for most general-purpose C/C++ programs.
%

Even if dynamic checks for data flows are minimized, handling live-out memory
state after parallel invocation may then become a bottleneck.
%
This problem is aggravated in the presence of cross-iteration false dependences
caused by data structures reuse.
%
To address this, speculative systems for automatic parallelization often need to
monitor large write sets, significantly degrading their
profitability~\cite{johnson:12:pldi}.
%~\cite{kim:12:cgo,johnson:12:pldi} .  to ensure correct live-out state after
%parallel invocation.
%
%problem often hidden with mem spec
%
%Certain state-of-the-art speculative systems~\cite{kim:12:cgo,johnson:12:pldi}
%completely ignore this problem and employ naive approaches that require
%unncessary monitoring and communication for
%
%Process-based vs thread-based thread-based need to pay for copy-in and copy-out
%process-based only for copy-out.
%
%prior proposals often also need to monitor large write sets to support
%privatization; at the end of the parallel region, workers' memory states need
%to be merged correctly.  determine the last written values and produce a
%correct live-out memory state at the end of the parallel region.  merge
%correctly speculative states privatization

%
%the problem is bookeeping and communication for validation and handling
%speculative state and privatization.
%need speculation for RAW and privatization for output/anti (false) deps.
%not easy to privatize objects with pointers and hard to statically determinate
%layout (~\cite{Privateer}).
%Privateer is the only fully automatic system capable of privatizing data
%structures in languages with pointers,type  casts,  and  dynamic  allocation
%

%To attain the parallelization coverage of speculative techniques, while
%minimizing the usage of memory flow speculation and the cost of privatization,
%we propose reasoning using a collaboration of static analysis with
%cheap-to-validate speculative assumptions.
%
%we propose that static analysis works synergistically with cheap-to-validate
%speculative assumptions to infer properties that would not be discoverable in
%isolation.
%%
%Of these properties, prior work would have required memory speculation to reason
%about data flow related ones, and would have failed to reason about certain
%output dependence related ones. The latter are essential for efficient
%privatization, a key enabler of
%parallelization~\cite{7,21,22,29,32_from_privateer}.
%%

To attain the parallelization coverage of speculative techniques, while
minimizing the usage of memory flow speculation and the cost of privatization,
we propose a unified parallelization framework that employs fine-grained
combination of static analysis and cheap-to-validate speculative assumptions.
%, and enables high-level properties inference through the collaboration of
%static analysis with cheap-to-validate speculative assumptions.
%
The main distinguishing factor is that prior work uses static analysis and
speculative assumptions separately, each inferring different program properties
in isolation, while this work combines the strengths of static and
cheap-to-validate speculative analysis and enables resolution of multi-logic
queries.
%
%Individual static or speculative analysis algorithms yield a multiplicative
%improvement to the overall precision of the ensemble, rather than the additive
%improvements of the sum-of-parts approach of prior work.
%
%each reason for a different thing/logic.  infer properties that would require
%expensive runtime monitoring and checks.
%

Collaboration of static and cheap-to-validate speculative analysis enables, in
some cases, removal of memory dependences that would require, in prior work,
expensive memory flow speculation.
%
For example, control speculation asserts the absence of memory dependences to or
from speculatively dead operations, while kill-flow analysis disproves memory
dependences by finding killing operations along all feasible paths between two
operations. If kill-flow queries control speculation for speculative control
flow information, then they can collaboretively remove memory dependences which
neither one can remove in isolation.
%
This idea is inspired by the benefits of collaboration among static analysis
algorithms in CAF~\cite{johnson:17:cgo}.

Combining static with cheap-to-validate speculative analysis also enables
property inference for efficient privatization.
%
Prior parallelization systems
%~\cite{smtlite:09:pldi, kim:12:cgo, johnson:17:cgo}
use static analysis or speculation mainly to remove memory dependences.
Sometimes though some dependences are real and cannot be removed via analysis.
For example, reuse of data structures create cross-iteration false (output and
anti) memory dependences.
%
In this scenario, analysis passes and speculative information can still express
some useful properties for these dependences or the dependent instructions.
This partial information can be combined to infer a higher level property that
enables more efficient parallelization.
% isn't that what transformation do anyway? query analysis for properties?  but
% we present a structured way to express that?  how do analysis know what's
% useful.

% text from CAF
%Using  an  understanding  of  eachalgorithm’spartiality
%anddecomposition, these algorithmsisolateforeign premises—facts
%about the program which the analysis algorithm needs inorder to make further
%derivations,  yet which cannot be derived by this algorithm alone.A
%collaborative analysis formulates foreign premises in the native query language.
%Theensemble delegates thoseforeign premise queriesto other analysis algorithms
%to combinethe strengths of its members.

%we propose a unified parallelization framework that combines both static and
%speculative techniques.
%
%This combination is not as simple as using a static and a speculative compiler
%in a sequence~\cite{kim:12:cgo}; this simple setup would be undesirable as
%the limitations of each compiler are still present.
%
%We suggest that the selected parallelization plan for each target loop contains,
%if necessary, both speculative and conservative parallelization enabling
%transformations; the most efficient option for each parallelization obstacle is
%chosen.
%
%Similarly to Privateer, we believe that reasoning about high-level properties
%of memory patterns is cheaper than reasoning about individual mem ops.

%This paper presents LSD, a DOALL parallelization framework, that leverages
%enhanced program analysis through the collaboration of static and speculative
%analysis to enable more effective and scalable parallelization compared to prior
%work.
%
%a planning phase for  transformation selection Our proposed framework also
%involves a planner that carefully selects from a large set of conservative and
%speculative enabling transformations the most efficient option for each
%parallelization obstacle.
%

This paper presents LSD, an instantiation of this proposed framework for DOALL
parallelization. LSD includes a parallelizing LLVM-based compiler that selects
minimal-cost solutions to parallelization inhibitors using state-of-the-art
static analysis (\`{a} la CAF~\cite{johnson:cgo:17}) and speculative analysis
passes that interpret profiling-based speculative assumptions, and a lightweight
%, unified (for both speculative and non-speculative)
runtime system for efficient parallel execution and low-cost speculation
validation.
%

We focus on DOALL, because it is the ideal parallelization technique when
applicable. It is simple to orchestrate, delivers high thread-level parallelism
with low communication overheads, and it is the most studied parallelization
technique in literature by both pure static analysis-based
approaches~\cite{..,.,..} and speculation-based approaches~\cite{..,..,..,..}.
Further, its potential coverage is extensive when paired with
speculation~\cite{zhong:08:hpca}, but still results of prior work on real
hardware are underwhelming.

LSD achieves scalable automatic parallelization on commodity shared-memory
machines without any programmer hints.  We evaluate LSD on a set of 12 C/C++
benchmarks that have appeared in state-of-the-art automatic parallelization systems'
papers~\cite{johnson:12:pldi,kim:12:cgo,campanoni:12:cgo}. On a 28-core machine,
LSD yields a geomean whole-program speedup of 23.8$\times$ over sequential execution and
2$\times$ compared to a state-of-the-art spec-DOALL system~\cite{johnson:12:pldi}.
%better or as good as prior work
These speedups are thanks to effective usage of static properties of the code in
conjunction with cheap speculative assumptions,
%to avoid memory flow speculation and minimize privatization costs,
%
%careful selection of applied transformations
and a lightweight process-based runtime system.  No prior work
is able to parallelize all these benchmarks with such sparse usage of memory
speculation and so efficient and effective privatization; thus no prior work
could achieve comparable results.
