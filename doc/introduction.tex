% !TEX root = paper.tex
\section{Introduction}

Using PThreads~\cite{pthreads:spec}, Map-Reduce~\cite{dean:04:osdi},
OpenMP~\cite{openmp:spec}, and other libraries and languages,
programmers routinely produce coarse-grained parallel (CGP) programs
even at the warehouse scale.  At the other end of the parallelism
granularity spectrum, compilers and out-of-order processors
consistently extract instruction-level parallelism (ILP) from programs
without any programmer intervention.  Unfortunately, despite
developments in parallel programming languages, parallel libraries,
and parallelizing compilers, reliably finding parallelism appropriate
for multicore remains a challenge.  Programs with CGP are not ideally
suited for multicore as they tend to stress multicore's shared
resources, such as caches and memory bandwidth.  Google revealed, in a
candid disclosure, that their multicore utilization rarely exceeds
20\%~\cite{barroso:07:computer}. Other researchers have also made this
observation~\cite{chung:13:isca}.  Despite progress in recent years,
automatic parallelization is not yet a reliable solution.

Parallelizing compilers integrate program analysis,
enabling transformations and parallelization patterns to
find work that can execute concurrently.  Automatic parallelization
naturally focuses on loops because that is where programs spend their
time.  An essential aspect of program analysis in a parallelizing
compiler is memory analysis because the compiler must understand how
loops use memory objects to divide their work across threads
or processes.  Enabling transforms use control flow and data flow
facts from analysis to make the code amenable to a given
parallelization pattern.  Examples of enabling transform include loop
skewing which re-arranges array accesses to move cross-iteration
dependences out of inner loops, reduction which expands storage
locations to relax ordering constraints on associative and commutative
operations and privatization which creates private data copies for
each worker process to remove contention caused by the reuse of data
structures.  Many parallelization patterns exist, but among the most
desirable is DOALL, the independent execution of loop iterations.

For decades, parallelizing compilers only performed enabling
transformations that could be proven correct with respect to the facts
provided by static analyses.  While this approach showed some success
in scientific codes, the dependence upon memory analysis, an analysis
notorious for its imprecision, severely limited the applicability of
automatic parallelization.

Following on the success of speculation for extracting ILP,
speculation in automatic parallelization has gained traction in the
last decade.  Speculation allows the compiler to optimize for the
expected case.  The effect is dramatic since there are many
fewer dependences in practice than can be proved nonexistent by memory
analysis.  Memory flow speculation is a popular speculative enabling
transform that asserts the absence of flow dependences not manifested
(or manifested infrequently) during profiling, backing its assertions
with runtime checks to initiate misspeculation recovery when
necessary.  Another important speculative enabling transform is
speculative privatization proposed in
Privateer~\cite{johnson:12:pldi}.  With speculation,
Privateer is able to handle dynamic data structures even in the
presence of unrestricted pointers, a task that proved insurmountable
for non-speculative privatization techniques.

Despite the dramatic advance that speculation for automatic
parallelization represents, challenges remain that prevent widespread
adoption~\cite{cascaval:08:stmtoy:short,prabhu:03:ppopp,kelsey:09:cgo}.
While memory flow speculation is popular, its relaxed program
dependence structure comes with a high cost.  Even without any
misspeculation, validation of memory flow speculation requires
instrumenting memory operations on every iteration to log or
communicate speculative accesses to additional validation code.  For
larger regions with many speculation checks, the validation cost can
become prohibitively expensive, negating the benefits of the
parallelization.  Speculative privatization may also entail high
overheads, but in a different way.  Correctly merging the private memory
state of each parallel worker at the end of a loop invocation can
require speculative privatization systems to monitor large write sets
during execution, significantly degrading their
profitability~\cite{kim:12:cgo,johnson:12:pldi,rauchwerger:99:pds}.

The goal of this work is to produce a parallelizing compiler that
achieves the coverage of speculative parallelization techniques while
also reducing its costs.  The proposed system, called Per{\em
  spec}tive, is an automatic parallelization framework integrating a
speculation-aware memory analyzer, efficient variants of speculative
privatization, and a parallelization planner to discover the best
performing set of transforms and avoid certain unnecessary
speculation overheads of prior work.

In Per{\em spec}tive, the speculation-aware memory analyzer helps the
parallelization planner understand the full impact of a speculative
assertion.  In practice, Per{\em spec}tive often recognizes that an
inexpensive speculative enabler addresses problems thought by prior
systems to require additional and more expensive speculation.  For
example, observing the pruning of a path never taken in profiling by
control-flow speculation, the speculation-aware memory analyzer knows
to remove memory dependences that exist as a result of the existance
of the pruned path.  In contrast, prior work systems are unaware of
these impacts and unnecessarily employ additional speculative
techniques to remove those memory dependences.

Per{\em spec}tive's planner makes adding new specutive enablers feasible.  The
planner, with information with the speculation-aware memory analyzer,
selects speculative enablers based on their cost and overall impacts.
Prior work systems were simply not sophisticated enough to make these
decisions in an informed way.  As a consquence, these systems
generally had a small numbrer of powerful, but expensive to validate,
speculative enablers.

The primary contributions of this paper are:
\begin{itemize}

\item The first \textbf{speculation-aware} memory analyzer that takes
  full advantage of speculation by allowing memory analysis to
  interpret speculative assertions as program facts;

\item New efficient \textbf{speculative privatization transformations}
that avoid the overheads of prior speculative privatization
techniques;

\item A \textbf{planning} phase that combines non-speculative
and speculative techniques to select the most profitable set of
parallelization-enabling transformations;

\item A \textbf{fully automatic} speculative DOALL parallelization
framework for \textbf{commodity hardware}, that exhibits
\textbf{scalable} speedups by minimizing the speculative
parallelization overheads of prior work.

\end{itemize}

\name achieves scalable automatic parallelization on commodity
shared-memory machines without any programmer hints.  We evaluate
\name on a set of 12 C/C++ benchmarks used in prior state-of-the-art
automatic parallelization system
papers~\cite{johnson:12:pldi,kim:12:cgo,simone:12:cgo}. On a 28-core
machine, \name yields a geomean whole-program speedup of 23.0$\times$
over sequential execution.  This represents a doubling in performance
compared to Privateer, the most applicable prior state-of-the-art
speculative DOALL system~\cite{johnson:12:pldi}.  These results come
from the effective usage of static properties of the code in
conjunction with cheap speculative assertions, the careful selection
of applied transformations, and a lightweight process-based runtime
system.  Per{\em spec}tive represents an important advance in
fulfilling the promise of automatic parallelization.



%PriorNo prior work is able to parallelize all these benchmarks with
%such sparse usage of memory speculation and efficient privatization;
%thus no prior work could achieve comparable results.

%Combining static analysis with cheap-to-validate speculative
%assumptions also enables efficient variants of speculative
%privatization.  These variants avoid the extensive bookkeeping
%observed in Privateer, the state-of-the-art scheme for speculative
%privatization~\cite{johnson:12:pldi}.  Privateer relies exclusively
%on profiling information to prove privatizability and thus needs to
%dynamically detect the last written value of each privatized byte.
%The need for speculation to privatize should not exclude, though, the
%use of static analysis to infer additional properties that minimize
%the privatization overheads.

%This paper presents \namensp, an instantiation of this proposed
%framework for DOALL parallelization. \name consists of a
%parallelizing LLVM-based compiler and a lightweight , unified (for
%both speculative and non-speculative execution) runtime system. The
%compiler selects minimal-cost solutions to parallelization inhibitors
%using state-of-the-art static analysis
%(e.g. CAF~\cite{johnson:14:pldi}) and profiling-based speculative
%assumptions, while the runtime offers efficient parallel execution
%and low-cost speculation validation.

%We focus on DOALL because it is the ideal parallelization technique
%when applicable. It is simple to orchestrate, delivers high
%thread-level parallelism with low communication overheads, and it is
%the most studied parallelization technique in literature both by pure
%static analysis-based
%approaches~\cite{suif:94:stanford,blume:94:polaris,rus:03:hybrid,rauchwerger:94:ics}
%and speculation-based
%approaches~\cite{rauchwerger:99:pds,dang:02:ipdps,mehrara:09:stmlite,kim:12:cgo,johnson:12:pldi}.
%Further, its potential coverage is extensive when paired with
%speculation~\cite{zhong:08:hpca}, but still, results of prior work on
%real hardware are underwhelming.

