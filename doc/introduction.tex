\section{Introduction}

% SIMONE: title = I would change it to emphasize the main novelty of this work, which is merging static analysis and speculation to reach more than the sum of the parts

% SIMONE: version 1 of Section 1: overall feedback = I think it improved since version 0. However, I think it is too long, but there are opportunities to shrink it considerably. This will bring the description of the novelty of the paper much sooner. For example, consider the paragraph about privateer. It doesn't look like we need that argument for introducing the novelty of this paper. Hence, that paragraph can go to Section 2 (Background and Motivation). Also, I would emphasize much more the fact that merging static analysis with speculation results to more than the sum of the parts. Then, I would describe the intuition behind this claim and, finally, the results of this fact (e.g., no prior work is able to parallelize all benchamrks we do parallelize in this paper).

% SIMONE: version 0 of Section 1: overall feedback = I like the flow, but I think it would be nice to (i) anticipate the novelty of this paper and (ii) highlight much more it. For example, early in the introduction we can say that today's approaches are either speculative without the use of static analysis or conservative without the use of speculative techniqeus. Both approaches have advantages and disadvantages. We are the first ones to put them together to leverage their advantages and avoid their weaknesses.

% AutoPar is important.
%Coincident with the introduction of multicore, the processor industry has
%fallen well short of the decades-old single-threaded performance growth
%trend. The burden of extracting sufficient multicore-appropriate parallelism
%to overcome this sequential performance deficit has fallen on programmers
%and compilers. Yet, despite many new tools, languages, and libraries
%designed for multicore, the difficulty of parallelism extraction keeps
%multicore grossly underutilized.

%Parallelization is necessary

Multicore architectures are ubiquitous in systems ranging from mobile to
``warehouse scale" systems. Thus, parallelism is more important than ever to
harvest more performance from current hardware.  This burden of extracting
sufficient parallelism has fallen on programmers and compilers. Yet, despite
many new tools, languages, and libraries designed for multicore, multicore
remains a grossly underutilized technology.


%Programmers?

%There are many ways programmers can manually realize parallelism.  Common
%approaches, such as PThreads~\cite{pthread:web},
%Map-Reduce~\cite{dean:08:cacm}, and OpenMP~\cite{openmp:web}, generally help
%programmers extract coarse grained parallelism (CGP) from hot loops in
%applications.
Manually expressing parallelism is tedious and error-prone. The programmers that
do attempt parallelism typically rely on popular approaches (e.g.,
PThreads~\cite{pthread:web}, Map-Reduce~\cite{dean:08:cacm}, and
OpenMP~\cite{openmp:web}) that focus on extracting coarse grained parallelism
(CGP) from hot loops in applications.
%
%CGP can be extremely beneficial in cluster and warehouse-scale systems, but has
%limitations on multicore.
Programs with CGP are not ideally suited though for multicore as they tend to
stress multicore's shared resources, such as caches and memory bandwidth.
Google revealed, in a candid disclosure, that their multicore utilization rarely
exceeds 20\%~\cite{barroso:07:computer}. Other researchers have also
subsequently corroborated this observation~\cite{chung:13:isca}.
%
To make matters worse, for programs with complex control and data flow, even CGP
extraction is often extremely difficult.
%
Asking programmers to address this underutilization with finer-grained
parallelism extraction would result in enormous increase in programmers' effort.
%

%can be very difficult, and most such programs remain without a CGP extracted
%version.
%
% SIMONE: should we say, instead, that these programs either don't have CGP at
% all (e.g., GCC) or what they have is enough for a few cores only and it is a
% weak-scaling rather than strong scaling (e.g., Chrome has CGP where one tab is
% assigned to one thread; the more tabs, the more threads, the more CGP => weak
% scaling)?

%autopar is very useful
Automatic parallelization is an attractive approach to make efficient use of
multicore systems, yielding performance improvement even for already parallelized
applications (e.g, reduced maximum latency on each parallel task of a
CGP-parallelized application). Unfortunately, even after 15 years of multicore,
automatic parallelization is still mostly limited to scientific and
data-intensive applications.
%the routine extraction of TLP remains elusive.

%analyis limitations
Major impediments to automatic parallelization are the limitations of static
analysis. Static dependence analysis (including alias analysis and control flow
analysis) are undecidable and difficult in practice, so compilers rely upon
imprecise static analysis that limits parallelizing compilers applicability to
regular code patterns (e.g., scientific applications) and small loops, or forces
expensive inter-thread communication that severely hinders scalability.
%TODO: ADD citations to non-spec parallelizing compilers.  is there a reference
%that shows that HELIX byitself does not scale to more cores?  cite PS-DSWP and
%doall non-spec parallelization (e.g., SUIF)
%
%Static analysis alone is simply too imprecise to enable aggressive
%parallelization.
In addition to being conservative, static analysis describes program facts that
are true independently of the input, resulting in missed opportunities for
parallelizing compilers.
%the dependence patterns of real applications are frequently driven by the
%program input, or experience a phase change during program execution
Current static parallelizing compilers~\cite{campanoni:2012:iscgo,
raman:2008:iscgo, suif:94:stanford}
% TODO: cite non spec DOALL
when applicable can deliver good
predictable speedups, but they all suffer from the aforementioned
limitations, and consequently fail to enable aggressive and scalable
parallelization for most general-purpose applications.

%speculation necessary
Speculation allows the compiler to overcome static analysis' constraints,
optimize for an expected
%/common
case, and ignore rare or impossible dependences.  Research into speculation has
shown incredible potential for the applicability and aggressiveness of
parallelizing transformations~\cite{zhong:08:hpca, johnson:12:pldi:short}.
%Speculative parallelization exploits simplifying assumptions about the input
%code to deliver increased parallel performance.
%
%speculation problems
Yet, speculative runtime systems face real challenges that prevent their
widespread adoption~\cite{cascaval:08:stmtoy:short}.
% TODO: Add more citations here
Even if we assume
that misspeculation never occurs, validating speculative assumptions during
a speculative region may heavily degrade performance.
%
Most speculative systems perform this validation by instrumenting operations
within the parallel region to log or communicate speculative events to a
validation system.  These validation checks add to parallel region latency in
the common case, not only during recovery.

Many proposals speculate low-level properties of the code, namely dependences
between individual operations.
%TODO: Cite HW TLS Papers
Such speculative systems work well when assisted by hardware
extensions, but yield small speedups, if any, when
running on commodity hardware.
%TODO: other software-spec apart from STM, more recent ones ??
For example, Software Transactional Memories
(STMs)~\cite{mehrara:09:stmlite} present an abstraction that facilitates
speculating the independence of memory transactions (i.e., memory
speculation). Validation requires logging or communication for most of the
memory accesses within each transaction and becomes prohibitively expensive
as the size of transaction grows.
%
%This reduces to comparing the read and write sets of adjacent transactions.  As
%transactions grow, the number of memory operations within that transaction can
%become prohibitively large. Further, instrumenting every memory operation with
%the transaction to log or communicate every access---approximately one tenth of
%all dynamic instructions---leads to an excessive overhead, even in the absence
%of transaction rollbacks.

%privateer
Other speculative systems attempt to avoid the excessive memory speculation of
generic transactional systems by speculating higher-level properties whose
validation does not require logging or communication.
%
A prominent work of this class is Privateer~\cite{johnson:12:pldi:short}, a
state-of-the-art Spec-DOALL parallelization system, that exhibits more
scalable speedups compared to prior work.
%
Privateer partitions memory objects into several categories/families according
to observed access patterns via profiling.  Speculating that certain
individual memory access pairs are independent is avoided by just speculating
separation of the families and some other simple properties.  However, detection
of privatizable memory objects relies solely on memory and control profiling.
Therefore, validation for a substantial set of memory objects still requires
expensive logging and checks at every memory access, similarly to STM systems.
% and occassional communication among workers
%cheap spec for short-lived or read-only has also been discussed in other spec
%systems such as cluster-doall and corD
%

The common observed pattern that limits the gains of speculative systems is
needless usage of expensive-to-validate speculation and insufficient, if any,
leverage of statically provable properties of the code.

To attain the coverage of speculative techniques while minimizing usage of
expensive speculation,
%expensive memory speculation.
we propose a unified parallelization framework that combines both static and
speculative techniques.
% TODO: add more citations here
This combination is not as simple as using a static and a speculative compiler
in a sequence~\cite{kim:12:cgo}; this simple setup would be undesirable as
the limitations of each compiler are still present.
%
We suggest that the selected parallelization plan for each target loop contains,
if necessary, both speculative and conservative parallelization enabling
transformations; the most efficient option for each parallelization obstacle is
chosen.
%
% Sadly, this could be true in principle given our remediators but there is no
% such example in our current benchmarks.
%In terms of ordering, an intuitive approach would be to consider all the
%conservative tranformations before the speculative ones. We disprove this
%intuition for certain cases. We propose instead that all techniques are
%considered together and that the more efficient option for each parallelization
%obstacle is chosen.
%

Our unified proposed framework also involves enhanced program analysis though
the coexistence of static analysis and speculation. In prior work, static
analysis and speculative assumptions are often considered separetely, each
disproving different memory dependences. Instead, we propose that static analyis
works synergistically with, cheap-to-validate, speculative assumptions to infer
program properties that would not be discovered in isolation. This significantly
minimizes the usage of memory speculation of prior work.
%similarly to how collaboration of static analysis passes works instead of
%additive, multiplicative effect work synergistically together ctrl
%spec+killflow, value_pred+alias analysis Privatization is a huge issue, prior
%work either did static detection, or detection via memory profiling or asusmed
%certain memory layout.



%Similarly to Privateer, we believe that reasoning about high-level properties
%of memory patterns is cheaper than reasoning about individual mem ops.

This paper presents LSD, an instantiation of this proposed framework for DOALL
parallelization. We focus on DOALL parallelization, as a first step, since it is
a simple, yet very effective~\cite{zhong:08:hpca}, and well-studied
parallelization technique.
% TODO: need to fix the bib
% SIMONE = I would use a different argument for why we chose DOALL. I would say that DOALL is the ideal parallelization, because it is simple to orchestrate, brings high TLP with low overhead. Finally, it is the most studied parallelization technique in literature by both pure static analysis-based approaches and speculation-based approaches.
LSD consists of a state-of-the-art static analysis (CAF~\cite{..}) that
maximizes statically disproven loop-carried dependences, a set of profilers that
generate speculative assumptions, a parallelizing LLVM-based compiler that
selects minimal-cost solutions to parallelization inhibitors from a wide range
of enabling transformations, and a lightweight, unified (for both speculative
and non-speculative) runtime system for efficient parallel execution and
low-cost speculation validation.
%

LSD achieves scalable automatic parallelization on commodity shared-memory
machines without any programmer hints.  We evaluate LSD on a set of 12 C/C++
programs that appear on prior DOALL parallelization systems' papers. On a
28-core machine, LSD yield a geomean whole-program speedup of Xx over sequential
execution and Yx compared to Privateer.
%better or as good as prior work
These speedups are thanks to effective usage of static properties of the code in
conjunction with cheap speculative assumptions, careful selection of applied
transformations and a lightweight process-based runtime system.  No prior work
is able to parallelize all these benchmarks with such sparse usage of memory
speculation, thus no prior work could achieve comparable results.
