% !TEX root = paper.tex
\section{Introduction}

\begin{table*}[t]
  \include{figures/related-work-tab}
  \caption{
    Comparison of \name with Automatic DOALL software-only systems.
  }
  \label{tab:related-work}
    \vspace{-5pt}
\end{table*}

Multicore architectures are ubiquitous in systems.  Thus, extracting
parallelism is more important than ever to harvest more performance
from current hardware.
%
%This burden of extracting sufficient parallelism has fallen on
%programmers and compilers.
Yet, despite many new tools, languages, and libraries designed for
multicore, multicore remains a grossly underutilized technology.

%Programmers?
%
Manually expressing parallelism is tedious and error-prone. The
programmers that do attempt parallelism typically rely on popular
approaches (e.g., PThreads~\cite{pthread:web},
OpenMP~\cite{openmp:web}, Map-Reduce~\cite{dean:08:cacm}) that focus
on extracting coarse grained parallelism from programs with simple
control and data flow. This is beneficial for various workloads in
cluster and warehouse-scale systems but is insufficient for general
purpose programs on multicore, where finer-grained parallelization is
needed.

%Even if manual parallelization is successfully applied, %a program is
%manually parallelized, it is still an unappealing option due to the
%high maintenance cost of parallel code compared to sequential (five
%times more expensive according to Google~\cite{google_cite_simone}).
%


%autopar is very useful
Automatic parallelization is a more attractive approach to make
efficient use of multicore systems.
%Unfortunately, even after 15 years of multicore, automatic
%parallelization is still mostly limited to scientific and
%data-intensive applications.  the routine extraction of TLP remains
%elusive.
%
Automatic parallelization techniques utilize analysis and enabling
transformations attempt to distribute, without data races, work across
processes or threads.
%
Analysis infers program properties, while enabling transformations
modify the program to eliminate parallelization inhibitors identified
by the analysis.
%
Such a parallelization inhibitor is reuse of data structures.
Privatization, a key enabling transformation for
parallelization~\cite{citations_from_privateer}, can remove contention
caused by this reuse by creating a private copy for each parallel
worker.

%analyis limitations
Early works on automatic parallelization had limited applicability -
restrained to regular code patterns (e.g., scientific applications) -
due to their reliance on static dependence analysis (including alias
analysis and control flow analysis), which is undecidable and
difficult in practice, especially for general-purpose C/C++ programs.

%speculation necessary
Speculation allows the compiler to overcome static analysis'
constraints, optimize for an expected case, and ignore rare or
impossible dependences.  Apart from removing rare dependences,
speculation also led to the design of speculative versions of enabling
transformations that exhibit higher applicability than their
conservative counterparts.  One such example is a highly applicable
speculative privatization proposed in a system called
Privateer~\cite{johnson:12:pldi}.
%research into speculation has shown incredible potential for the
%applicability and aggressiveness of parallelizing
%transformations~\cite{zhong:08:hpca, johnson:12:pldi}.
%
%speculation problems
Yet, despite their potential, software speculative systems face real
challenges that prevent their widespread
adoption~\cite{cascaval:08:stmtoy:short, .., ..}.
% TODO: Add more citations here
%
%Major bottlenecks include high communication and bookkeeping costs
%for memory flow speculation validation, and memory live-out handling.
%and privatization.  The observed pattern is the more applicable
%speculation made, the less efficient it became.

The relaxed program dependence structure offered by memory flow
speculation comes with a high cost.  Even if we assume that
misspeculation never occurs, validation of memory flow speculation
requires instrumenting memory operations within the parallel region to
log or communicate speculative accesses to a validation system. As the
size of the speculative region increases, the validation cost becomes
prohibitively expensive and outweighs any parallelization benefit.

Similarly, speculative enabling transformations, and
%most prominently
in particular speculative privatization may also entail high
overheads.  Merging the private memory state of workers to the
committed memory state often forces speculative systems to monitor
large write sets during parallel execution, significantly degrading
their profitability~\cite{kim:12:cgo,johnson:12:pldi}.
%to ensure correct live-out state after parallel invocation.

To attain the parallelization coverage of speculative techniques,
while minimizing the usage of memory flow speculation and the cost of
privatization,
%
this work proposes an automatic parallelization framework which
combines a speculation-aware memory analyzer, efficient variants of
speculative privatization, and a parallelization planner. These
components during an exploration phase discover the best performing
set of parallelization techniques. This way, certain unnecessary
parallelization overheads of prior work are avoided.
%
An important distinguishing factor is that prior work uses static
analysis and speculative assumptions separately, each inferring
different program properties in isolation, while this work combines
the strengths of static analysis and cheap-to-validate speculative
assumptions to enable more efficient speculative parallelization.

Making memory analysis aware of cheap-to-validate speculative
assumptions occasionally enables removal of memory dependences that
would require, in prior work, expensive memory flow speculation.
%
For example, edge profiling detects never taken branches, while
kill-flow analysis disproves memory dependences by finding killing
operations along all feasible paths between two operations. If
kill-flow is able to use speculative control flow information, then it
can assert absence of, non statically disprovable, memory dependences
under a cheap-to-validate speculative assumption.
%This idea is inspired by the benefits of collaboration among static
%analysis algorithms in CAF~\cite{johnson:17:cgo}.
Note, that by just applying control speculation and then re-running
memory analysis on the transformed code would not have the same
effect.  Memory analysis needs to treat this speculative information
as a fact and ignore the possibility of misspeculation.

Combining static analysis with cheap-to-validate speculative
assumptions also enables efficient variants of speculative
privatization.
%
These variants avoid the extensive bookkeeping, during parallel
execution, that is observed in the state-of-the-art scheme for
speculative privatization~\cite{johnson:12:pldi}.
%
The difference is that this prior work relies exclusively on profiling
information to prove privatizability and thus needs to dynamically
detect the last written value at each privatized byte.
%
The need for speculation to privatize should not exclude, though, the
use of static analysis to infer additional properties that minimize
the privatization overheads.

This paper presents \name, an instantiation of this proposed framework
for DOALL\footnote{loop iterations run independently of each other}
parallelization. \name consists of a parallelizing LLVM-based compiler
and a lightweight
%, unified (for both speculative and non-speculative execution)
runtime system. The compiler selects minimal-cost solutions to
parallelization inhibitors using state-of-the-art static analysis
(\`{a} la CAF~\cite{johnson:cgo:17}) and profiling-based speculative
assumptions, while the runtime offers efficient parallel execution and
low-cost speculation validation.

We focus on DOALL, because it is the ideal parallelization technique
when applicable. It is simple to orchestrate, delivers high
thread-level parallelism with low communication overheads, and it is
the most studied parallelization technique in literature by both pure
static analysis-based approaches~\cite{..,.,..} and speculation-based
approaches~\cite{..,..,..,..}.  Further, its potential coverage is
extensive when paired with speculation~\cite{zhong:08:hpca}, but still
results of prior work on real hardware are underwhelming.

The primary contributions of this paper are:
\begin{itemize}

\item The first \textbf{speculation-aware} memory analyzer that takes
full advantage of speculation by allowing memory analysis to interpret
cheap-to-validate speculative assumptions as facts;
%combined strenghts

\item A novel efficient \textbf{speculative privatization transformations}
that avoid the overheads of prior speculative privatization
techniques;

\item A novel \textbf{planning} phase that combines non-speculative
and speculative techniques to select the most profitable set of
parallelization-enabling transformations; and,

\item A \textbf{fully automatic} speculative DOALL parallelization
framework for \textbf{commodity hardware}, that exhibits
\textbf{scalable} speedups by minimizing the speculative
parallelization overheads of prior work;

\end{itemize}

\name achieves scalable automatic parallelization on commodity
shared-memory machines without any programmer hints.  We evaluate
\name on a set of 12 C/C++ benchmarks that have been used in
state-of-the-art automatic parallelization systems'
papers~\cite{johnson:12:pldi,kim:12:cgo,campanoni:12:cgo}. On a
28-core machine, \name yields a geomean whole-program speedup of
23.0$\times$ over sequential execution and 2$\times$ compared to a
state-of-the-art speculative DOALL system~\cite{johnson:12:pldi}.
These speedups are thanks to effective usage of static properties of
the code in conjunction with cheap speculative assumptions,
%to avoid memory flow speculation and minimize privatization costs,
%
careful selection of applied transformations and a lightweight
process-based runtime system.  No prior work is able to parallelize
all these benchmarks with such sparse usage of memory speculation and
so efficient and effective privatization; thus no prior work could
achieve comparable results.
