\section{Related Work}

% For non-spec DOALL-only compilers: with only static analysis, the
% struggle is the applicability (maybe plus the cost of logging and merging
% live-outs (copy-outs)); with runtime checks (predicates), the focus is
% the applicability and extraction of efficient runtime checks.

% For compilers using different techniques than DOALL: communication of
% real dependences is needed. The overheads can be broken down to
% communication cost, lost cycles due to imbalance.

% For Spec DOALL-only compilers: with memory speculation, the applicability
% is perfect, meaning all “real” DOALL loops can be handled by just
% ignoring dependences that can not be disproved but will never or seldom
% manifest in runtime. The overheads include speculation validation cost,
% misspeculation recovery cost, and the cost of logging & merging
% (maintaining) live-outs.

% For systems with special hardware support: for example, with
% transactional memory systems, the memory versioning of main memory is
% there implicitly. Assuming you have a transactional memory system with
% zero time overhead, what you need to do is just spawning the iterations,
% false dependences including anti- and output- dependences are avoided,
% when one iteration has a real dependences (flow) to its previous
% iteration but executes earlier, the memory system can stall it when it
% tries to load the pending value. However, the assumption of 0-overhead TM
% systems is not possible.

% \paragraph{Analysis-based Parallelization System}
% % Polaris
% % SUIF
% % HELIX
% % DSWP (non spec version)
% % Hybrid Analysis
% % Sensitivity Analysis

Early non-speculative parallelizing compilers (Polaris, SUIF) based on
DOALL paradigm use static analysis to disprove dependences and run-time
analysis to extract predicate for dynamic privatization. These systems work
well with regular scientific programs and exhibit good speedup when
applicable. However, when dealing with general purpose programs, they are
limited by imprecise static analysis and heavyweight run-time analysis.
They also lack in support for dynamically allocated memory objects, which
are widely used in modern programming. Recent works on non-speculative
parallelization (HELIX, DSWP, and PS-DSWP) focus more on alternative
parallelization paradigms like DOACROSS and DOPIPE, which allow
communication of flow dependences among cores. \name focuses on DOALL but
the contributions of this work are complementary with other parallelization
paradigms. LRPD leverages speculation to evaluate privatization criterion,
extends the applicability of DOALL. Yet it is still limited to array-based
objects.

STMlite, CorD, and Privateer use speculation extensively to privatize
objects for general purpose programs. As mentioned in Privateer, STMlite
and CorD don't support speculative reduction, and limit the applicability
to static cases. Privateer, by introducing heap separation and doin
aggressive speculative privatization, achieves the state-of-the-art
applicability. However, it fails to recognize a lot of optimization
opportunities and suffers from the two major overhead mentioned in
\ref{sec:motivation}.

Recent work~\cite{ctian:2008:micro,johnson:12:pldi,kim:12:cgo} avoids some
memory speculations by speculating higher-level properties like local
objects whose validation does not require logging or communication.
Sensitivity Analysis~\cite{Rus:07:ics} uses a cascade design of run time
tests to generate predicates for memory dependences, which reduce the
run-time overhead. Yet, all of these optimization efforts are rudimentary
compared to \namensp.

Another line of work extracts parallelism among loop iterations by ignoring data dependences rather than speculating their non-existence~\cite{campanoni:2015:cgo,Udupa:2011:AEB:1993498.1993555,misailovic2013parallelizing}.
These approaches extract parallelism sacrificing the program's output quality.
Instead, \name extracts parallelism while guaranteeing the preservation of the original output quality.

Finally, some recent work speculate on properties of data dependences like commutativity~\cite{kulkarni:07:pldi,Nguyen:2014:DGO:2541940.2541964} and having short memory~\cite{Deiana:2018:UPN:3173162.3173181}.
These techniques involve heavy speculation and they require developers to cast programs in specialized code patterns expected by their compilers and runtimes.
Instead, \name parallelization is transparent to developers allowing them to write their code how they prefer to target their specific goals (e.g., productivity, maintainability, integrability).

% HELIX
% The HELIX parallelizing compiler extracts TLP by distributing loop
% iterations between cores within the same chip. HELIX is a generalization of
% DOACROSS and DOALL techniques for modern multicore architectures and,
% therefore, HELIX-parallelized loops include DOALL ones. HELIX does not rely
% on speculation to avoid the related overhead, which limits the approach to
% target only medium and small loops (where the accuracy of code analyses is
% higher) for most benchmarks. Targeting such loops increases the
% communication demand, which makes HELIX particularly appealing when coupled
% with an architecture support designed to accelerate core-to-core
% communication. On the other hand, LSD decreases the speculation overhead
% enough to make DOALL parallelization practical on existing commodity
% multicores.

% Parallelization with Programmers' Support
% Paralax


% \paragraph{Speculative Parallelization System:}


% \paragraph{Software transactional memory system:}

% Other work on software transactional memory tried to reduce the overhead by
% using some extent of static analysis or/and privatization (STMlite, CorD).


% \paragraph{Attempts to Reduce Runtime Overheads}



