% !TEX root = paper.tex
\section{Related Work}

% For non-spec DOALL-only compilers: with only static analysis, the
% struggle is the applicability (maybe plus the cost of logging and merging
% live-outs (copy-outs)); with runtime checks (predicates), the focus is
% the applicability and extraction of efficient runtime checks.

% For compilers using different techniques than DOALL: communication of
% real dependences is needed. The overheads can be broken down to
% communication cost, lost cycles due to imbalance.

% For Spec DOALL-only compilers: with memory speculation, the applicability
% is perfect, meaning all “real” DOALL loops can be handled by just
% ignoring dependences that can not be disproved but will never or seldom
% manifest in runtime. The overheads include speculation validation cost,
% misspeculation recovery cost, and the cost of logging & merging
% (maintaining) live-outs.

% For systems with special hardware support: for example, with
% transactional memory systems, the memory versioning of main memory is
% there implicitly. Assuming you have a transactional memory system with
% zero time overhead, what you need to do is just spawning the iterations,
% false dependences including anti- and output- dependences are avoided,
% when one iteration has a real dependences (flow) to its previous
% iteration but executes earlier, the memory system can stall it when it
% tries to load the pending value. However, the assumption of 0-overhead TM
% systems is not possible.

% \paragraph{Analysis-based Parallelization System}
% % Polaris
% % SUIF
% % HELIX
% % DSWP (non spec version)
% % Hybrid Analysis
% % Sensitivity Analysis

Early non-speculative DOALL parallelizing compilers
(Polaris\cite{blume:94:polaris},
SUIF\cite{amarasinghe:93:pldi,suif:94:stanford} are limited by imprecise
static analysis and heavyweight runtime analysis. They also lack
support for dynamically allocated memory objects, which are widely used in
modern programming. Recent works on non-speculative parallelization
(HELIX~\cite{simone:12:cgo}, DOCyclical~\cite{yu2016cyclical},
DSWP~\cite{ottoni:05:micro}, and PS-DSWP~\cite{raman:08a:cgo}) focus more
on alternative parallelization paradigms like DOACROSS and DOPIPE, which
allow communication of flow dependences among cores. \name focuses on DOALL,
but the contributions of this work are complementary with other
parallelization paradigms.

LRPD\cite{rauchwerger:99:pds} and R-LRPD\cite{dang:02:ipdps} leverage
speculation to evaluate privatization criterion, extending the applicability
of DOALL. Yet they are still limited to array-based objects.
STMlite~\cite{mehrara:09:stmlite}, CorD~\cite{tian:10:pldi}, and
Privateer~\cite{johnson:12:pldi} use speculation extensively to privatize
objects for general-purpose programs. As mentioned in Privateer, STMlite
and CorD do not support speculative reduction, limiting their applicability
to static cases. Privateer, by introducing heap separation, achieves the
state-of-the-art applicability. However, compared to \namensp, it fails to
recognize a lot of optimization opportunities and suffers from the two
major overhead mentioned in~\cref{sec:motivation}.

Recent works~\cite{ctian:2008:micro,johnson:12:pldi,kim:12:cgo} speculate
higher-level properties like local objects whose validation do not
require logging or communication. SPEC-DSWP~\cite{vachharajani:07:pact}
over-speculates and undoes unnecessary speculation afterwards. Yet, all of
these optimization efforts focus on removing some overheads in an ad hoc
way which may miss opportunities due to lack of information, while
\name takes a sensible approach to explore the design space using a
planner.

% Another line of work extracts parallelism among loop iterations by ignoring
% data dependences rather than speculating their
% non-existence~\cite{campanoni:2015:cgo,Udupa:2011:AEB:1993498.1993555,misailovic2013parallelizing}.
% These approaches extract parallelism sacrificing the program's output
% quality. Instead, \name extracts parallelism while guaranteeing the
% preservation of the original output quality.

Finally, some recent work speculate properties of data dependences like
commutativity~\cite{kulkarni:07:pldi,Nguyen:2014:DGO:2541940.2541964} and
having short memory~\cite{Deiana:2018:UPN:3173162.3173181}. These
techniques involve heavy speculation, and they require developers to cast
programs in specialized code patterns expected by their compilers and
runtimes. Instead, \name parallelization is transparent to developers
allowing them to write their code to target their specific goals (e.g.,
productivity, maintainability, integrability).


% SPEC-DSWP
% Over speculating and undoing speculation

% HELIX
% The HELIX parallelizing compiler extracts TLP by distributing loop
% iterations between cores within the same chip. HELIX is a generalization of
% DOACROSS and DOALL techniques for modern multicore architectures and,
% therefore, HELIX-parallelized loops include DOALL ones. HELIX does not rely
% on speculation to avoid the related overhead, which limits the approach to
% target only medium and small loops (where the accuracy of code analyses is
% higher) for most benchmarks. Targeting such loops increases the
% communication demand, which makes HELIX particularly appealing when coupled
% with an architecture support designed to accelerate core-to-core
% communication. On the other hand, LSD decreases the speculation overhead
% enough to make DOALL parallelization practical on existing commodity
% multicores.

% Parallelization with Programmers' Support
% Paralax


% \paragraph{Speculative Parallelization System:}


% \paragraph{Software transactional memory system:}

% Other work on software transactional memory tried to reduce the overhead by
% using some extent of static analysis or/and privatization (STMlite, CorD).


% \paragraph{Attempts to Reduce Runtime Overheads}



