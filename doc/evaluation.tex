% !TEX root = paper.tex
\section{Evaluation}

LSD is evaluated against 12 C and C++ benchmarks, covering all the DOALLable
(showing speedup instead of slowdown) benchmarks from two state-of-the-art
automatic DOALL parallelization papers~\cite{johnson:12:pldi,kim:12:cgo}. We
also include one more benchmark (179.art) from HELIX.

\begin{table*}
  \include{figures/benchmark-tab}
  \caption{
    DOALL Coverage and Experiment Setting of Benchmarks
  }
  \label{tab:benchmark-list}
    \vspace{-5pt}
\end{table*}

The machine is using two 14-core Intel Xeon CPU E5-2697 v3 processors (28 cores
total) running at 2.60GHz (turbo-boost disabled) with 756GB of memory. The
operating system is 64-bit Ubuntu 16.04.5 LTS with GCC 5.5 (version 5.5.0
20171010) and LLVM Compiler Infrastructure (version 5.0.2).


\subsection{Parallelization Performance}
Figures Needed:
\begin{itemize}
\item General Speed Plot: With different cores (1-28), all benchmarks
\item Speedup Comparison: 24 cores performance compared with Privateer
Runtime breakdown (how to minimize the overhead?)

\end{itemize}

\begin{figure*}[htp]
  \includegraphics[width=\textwidth]{figures/3mm-scale-crop}
  \caption{3mm Speedup over Different Numbers of Cores}
  \label{fig:3mm-scale}
\end{figure*}

\subsubsection{Effect of parallelization on vectorization}

instrumentation kills vectorization


\subsection{Static Analysis and Enablers}
Figures Needed:
\begin{itemize}
\item CAF: with and without; no topping; with only BasicLoopAA and ScevAA
\item Coverage: Spec-DOALL percentage of coverage of each benchmark
\item Enablers: Enablers used for each benchmark
\item Optimization level: Performance with different optimization levels
\end{itemize}

Discussion Needed:

Present in a table for all the benchmarks which enablers were used

\subsection{Overhead Breakdown}

\begin{figure*}[htp]
  \includegraphics[width=0.9\textwidth]{figures/overheads}
\end{figure*}
\begin{figure*}[htp]
  \includegraphics[width=0.9\textwidth]{figures/comparison}
\end{figure*}
Huge speedup for dijkstra is largely due to the cheaper "malloc" that
we use in our heaps. Using a similar malloc in the sequential version
yields ~16x speedup in dijkstra and insignificant difference in the
other benchmarks.

%\subsection{Misspeculation Analysis}
%\begin{figure*}[htp]
%  \includegraphics[width=0.9\textwidth]{figures/misspec}
%  \caption{Misspeculation performance across speculative benchmarks}
%  \label{fig:misspec}
%\end{figure*}
Figure \ref{fig:misspec} shows how misspeculation affects the
performance of the six speculative benchmarks with misspeculation rates of
0.1\% and 0.2\%. Since none of the benchmarks exhibit misspeculation on the
given input sets, we artificially inject misspeculation at the end of
every 1000 and 500 iterations, respectively, to observe the performance
degradation. The inputs for \texttt{179.art} were not large enough to have
enough iterations for misspeculation at these rates, so we perform a
weighted average of non-misspeculating and misspeculating runs to achieve
an average corresponding to the respective rates. These results demonstrate
that Perspective performs well with only high confidence speculation.
% As the iteration within a checkpoint that misspeculation
% occurs can affect the recovery cost, we inject these misspeculations in the
% middle of a checkpoint chunk (\textbf{XXX} find better wording) to have a more
% realistic simulation, assuming that programs can misspeculate at any iteration with a
% uniform distribution.

\subsection{Power Consumption}

Power and energy stats here

