% !TEX root = paper.tex
\section{Evaluation}
\label{eval}
\name is evaluated on a commodity shared-memory machine with two 14-core
Intel Xeon CPU E5-2697 v3 processors (28 cores total) running at 2.60GHz
(turbo-boost disabled) with 756GB of memory. The operating system is
64-bit Ubuntu 16.04.5 LTS with GCC 5.5 (version 5.5.0 20171010).
\namensp's compiler is implemented on the LLVM Compiler Infrastructure
(version 5.0.2)\cite{LLVM:CGO04}.


\name is evaluated against 12 C and C++ benchmarks, covering all the
parallelizable (exhibiting speedup) benchmarks from two
state-of-the-art automatic speculative-DOALL parallelization
papers(Privateer~\cite{johnson:12:pldi}, Cluster
Spec-DOALL~\cite{kim:12:cgo}) as well one more benchmark
(179.art) from HELIX~\cite{simone:12:cgo}, a non-speculative automatic
parallelization system.
We choose benchmarks that have been parallelized in prior work because this
work is focused on boosting the efficiency of automatic parallelization
rather than increasing its applicability.

% These benchmarks are from five benchmark suites (SPEC\cite{},
% PARSEC\cite{bienia:08:parsec}, PolyBench\cite{},
% MiBench\cite{guthaus:2001:iiwsc}, Trimaran\cite{trimaran:web}).
We modify the benchmarks from the Polybench and \texttt{dijkstra} from
MiBench to dynamically allocate previously static arrays in order to accept
command line-defined sizes in the same way as ~\cite{johnson:12:pldi, kim:12:cgo}.
% All PolyBench benchmarks and \texttt{dijkstra} from MiBench are modified
% to accept command line arguments and allocate arrays dynamically in
% the same way as in~\cite{johnson:12:pldi, kim:12:cgo}.
%This modification is used to create bigger inputs, necessary for
%separating profiling and evaluation input sets.
%
%for exhibiting speedups on 28 cores
%
Benchmark are profiled using small inputs, while all the
experiments presented in this section are conducted using different,
large evaluation inputs. The evaluation inputs are chosen to be large
enough for the sequential version to run for at least 10 minutes to
observe accurate measurements on 28 cores. Reported speedups are an average
of 5 runs to minimize the effect of the variance in execution time that
manifests between runs.


\begin{table*}[h]
  \centering
  \include{figures/benchmark-tab}
  \caption{
    Benchmark Details:
    (A) Percentage of
    execution time spent inside the parallelized loop(s) compared to
    that of the whole program. The theoretical speedup
    is calculated using Amdahl's Law with the assumptions of no overheads and
    running with 28 workers.
    (B) Number and percentage of cross-iteration dependences
    handled by Speculation-Aware Memory Analyzer (SAMA) that were unresolved by
    static analysis alone. Entries denotes by
    ``N/A'' indicate all dependences are handled by static analysis.
    (C) Number and percentage of objects covered by efficient speculative
    privatization transformations proposed
    in this work.
    (D) Private read and write sizes measured using the
    test input for each benchmark; v1 represents
    \name with only the planner; v2 represents \name with
    the planner and propsed enablers.}
  \label{tab:benchmark-list}
    \vspace{-5pt}
\end{table*}

\subsection{Parallelization Performance}

\begin{figure*}[ht]
  \centering
  \resizebox{0.90\textwidth}{!}{
  \includegraphics[width=\textwidth]{figures/multi-core-crop}
  }
  \caption{Fully Automatic Whole Program Speedup over Sequential}
  \label{fig:multi-core-scale}
\end{figure*}

Figure \ref{fig:multi-core-scale} presents whole program speedups for
all benchmarks running on one (1) to 28 cores. These speedups are
relatvie to the sequential performance of the original code, compiled
with \texttt{clang++ -O3}. \name achieves scalable performance on all the
benchmarks.

We replace all calls to \texttt{\textbf{malloc()}} inside a parallelized
loop with our own heap allocator. Our implementation of this allocator
does not track segments of memory that have been freed for later use in the
way most C/C++ runtime libraries do and as such, the overhead for dynamic
(de)allocation is drastically reduced for some benchmarks, namely
\texttt{swaptions} and \texttt{dijkstra}. This is apparent in the
super-linear \{\textbf{XXX} or better-than-theoretical\} speedup that we
observe for these benchmarks.

% enc-md5, swaptions, gemm, 3mm, 2mm, blackscholes, doitgen
\texttt{enc-md5}, \texttt{swaptions}, \texttt{gemm}, \texttt{2mm},
\texttt{3mm}, \texttt{blackscholes}, and \texttt{doitgen} show
close to linear scalability. This performance comes from most memory
objects being privatized with little or no overheads.

% dijkstra
\texttt{dijkstra}, having many calls to \texttt{\textbf{malloc()}}
inside the inner loops, shows a significant variance in
the runtime.

% 052.alvinn
\texttt{052.alvinn} exhibits a lower speedup (11$\times$ on 28
cores) than other the benchmarks, resulting from the inner loop
being chosen for parallelization. With this loop constituting 97.5\% of the
execution time of the whole program, the theoretical max speedup is 16.7$\times$
based on Almdahl's Law.

% 179.art
\texttt{179.art}, whose theoretical maximum speedup is 22.5$\times$, due to
the same effect. Note that HELIX\cite{simone:12:cgo} reports 4.2$\times$ speedup
on 6 cores while \name exhibits 6.5$\times$, demonstrating that an efficient
speculative approach can outperform even a static approach.

% correlation and covariance
\texttt{correlation} and \texttt{covariance} have WAW
dependence patterns which cannot be resolved by static analysis or cheap
privatization variants. As a result, the corresponding memory objects are
classified as private and need logging and merging for write sets,
resulting in lower speedups.

\begin{figure*}[ht]
  \includegraphics[width=\textwidth]{figures/compare-privateer}
  \caption{Whole Program Speedup Comparison among variants of \name and Privateer with 28 Cores}
  \label{fig:speedup-compare}
\end{figure*}

Figure \ref{fig:speedup-compare} compares \namensp's results
 with two
variants of Privateer and one variant of \namensp. The first bar shows the
performance of Privateer implemented with best practices.
Some peephole optimizations are used to reduce the use of memory
speculation and to hoist and combine memory checks out of the inner loop(s).
The speedup and runtime breakdown results match the description in the paper.
Although a state-of-the-art framework in terms of applicability, Privateer
misses opportunities to remove read checks as mentioned in
STMLite\cite{mehrara:09:stmlite} and Cluster Spec-DOALL\cite{kim:12:cgo}. To
improve upon Privateer, we utilize static analysis
including a stronger KillFlow analysis pass to remove read checks.
\texttt{2mm}, \texttt{3mm}, \texttt{doitgen}, \texttt{enc-md5}, and
\texttt{179.art} benefit from this improvement as shown in the second bar.
Note that \textbf{ref XXX} if all dependences are removed statically, no
checkpoint is needed in the runtime.

The third bar presents the results of \name without the cheap
privatization transformations proposed in \ref{}. Parallelization through
\name, which incorporates speculation-aware-memory-analysis and cheap
privatization transformations, is presented in the fourth bar, achieving
23.0$\times$ speedup compared to 11.5$\times$ by the original Privateer.


% \subsubsection{Effect of parallelization on vectorization}

% instrumentation kills vectorization


% \subsection{Static Analysis and Enablers}
% Figures Needed:
% \begin{itemize}
% % (no need, just discuss the use of CAF in previous sections) \item CAF:
% % with and without; no topping; with only BasicLoopAA and ScevAA

% % (no need, show in the benchmark table) \item Coverage: Spec-DOALL
% % percentage of coverage of each benchmark
% \item Enablers: Enablers used for each benchmark
% \item Optimization level: Performance with different optimization levels
% \end{itemize}

% Discussion Needed:

% Present in a table, which enablers were used for all benchmarks

\subsection{Overhead Breakdown}

% \begin{figure*}[htp]
%   \includegraphics[width=0.9\textwidth]{figures/overheads}
%   \label{fig:overheads}
%   \caption{Overhead comparison with various enablers disabled}
% \end{figure*}
% \begin{figure*}[htp]
%   \includegraphics[width=0.9\textwidth]{figures/comparison}
% \end{figure*}
% Huge speedup for dijkstra is largely due to the cheaper "malloc" that
% we use in our heaps. Using a similar malloc in the sequential version
% yields ~16x speedup in dijkstra and insignificant difference in the
% other benchmarks.
% Shown in Figure \ref{fig:overheads} is the overhead breakdown of each
% benchmark comparing \name with various enablers disabled. The useful work
% of \name is normalized to 100\%, with the others being scaled accordingly.
% These result demonstrate the correlation between the sum of the overheads
% to the speedup that can be achieved. \{\textbf{XXX} Something about
% how we remove dependences with redundant private read elimination,
% the new kill\_private heap, and collaboration \}
Table \ref{tab:benchmark-list} shows the overhead of private reads and
writes for each of the variants of Privateer and \namensp. These results
demonstrate the correlation between the overheads achievable speedup.
Redundant read elimination \{\textbf{XXX is this the right
name?}\} removes reads that can be statically be inferred to never fail,
reducing the read set to nothing for some benchmarks.
Using cheap speculation, we are able to move some of the private objects to
the heaps that do not require expensive logging and checks, which improves
the performance of \texttt{dijkstra} by a modest(?) amount.
Using collaboration removes the remaining unnecessary logs that cannot
be removed with static analysis alone.

For most of the benchmarks, checkpointing does not add any
significant (> 1\%) overhead; the exceptions are
\texttt{052.alvinn}, \texttt{correlation}, and \texttt{covariance}.
The inner loop of \texttt{052.alvinn} is chosen for parallelization --
because the useful work of the loop is small and runs for 60,000
iterations, checkpointing constitutes a considerable portion of the run
time, \textasciitilde20\%. For \texttt{covariance} and \texttt{correlation},
the speculation-aware memory analyzer could not avoid using memory
speculation and as such, the checkpoints need to merge large private sets
with an overhead of \textasciitilde10\% for both \{\textbf{XXX} Make sure
this is true\}.

\subsection{Misspeculation Analysis}
\begin{figure}[htp]
  \includegraphics[width=\columnwidth]{figures/misspec-crop}
  \caption{Misspeculation performance across speculative benchmarks}
  \label{fig:misspec}
\end{figure}
Figure \ref{fig:misspec} shows how misspeculation affects the
performance of the six speculative benchmarks with a misspeculation rate of
0.1\%. Since none of the benchmarks exhibit misspeculation on the
given input sets, we artificially inject misspeculation at the end of
every 1000 iterations to observe the performance degradation.
Note that for many benchmarks, speculation is only used
because analysis cannot disprove a dependence, even though manually
examining the source code reveals that the dependence will never manifest.
Thus, the only place misspeculation can possibly occur is during a heap
check.
% Note that misspeculation with \name is extremely unlikely, as
% static analysis can disprove many speculative assumptions and for many of
% the benchmarks, misspeculation is only possible during a heap check.
The inputs for \texttt{179.art} were not large enough to have
enough iterations for misspeculation at this rate, so we perform a
weighted average of non-misspeculating and misspeculating runs to achieve
an average corresponding to the respective rate.
These results demonstrate that \name performs well with only high
confidence speculation.
Systems that have larger overheads have a larger chance of speculation (since
they use more speculative assumptions), but we minimize the amount of speculation.
\{\textbf{XXX} Reword this\}
% As the iteration within a checkpoint that misspeculation
% occurs can affect the recovery cost, we inject these misspeculations in the
% middle of a checkpoint chunk (\textbf{XXX} find better wording) to have a more
% realistic simulation, assuming that programs can misspeculate at any iteration with a
% uniform distribution.

% \subsection{Power Consumption}

% Power and energy stats here

