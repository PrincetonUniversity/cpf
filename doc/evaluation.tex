% !TEX root = paper.tex
\section{Evaluation}

\name is evaluated on a commodity shared-memroy machine with two 14-core
Intel Xeon CPU E5-2697 v3 processors (28 cores total) running at 2.60GHz
(turbo-boost disabled) with 756GB of memory. The operating system is 64-bit
Ubuntu 16.04.5 LTS with GCC 5.5 (version 5.5.0 20171010) and LLVM Compiler
Infrastructure (version 5.0.2).

\name is evaluated against 12 C and C++ benchmarks, covering all the
DOALLable (showing speedup instead of slowdown) benchmarks from two
state-of-the-art automatic DOALL parallelization
papers~\cite{johnson:12:pldi,kim:12:cgo}. We also include one more
benchmark (179.art) from HELIX.

\begin{table*}
  \include{figures/benchmark-tab}
  \caption{
    DOALL Coverage and Experiment Setting of Benchmarks
  }
  \label{tab:benchmark-list}
    \vspace{-5pt}
\end{table*}

Each benchmark is profiled using a profiling input and all the following
experiments are conducted using a different testing input.

\subsection{Parallelization Performance}

Figure \ref{fig:multi-core-scale} presents who application speedups for all benchmarks
running with different numbers of cores. These speedups are relatvie to the
best sequential performance of the original code, compiled with
\texttt{clang++ -O3}.

% alvinn
For \texttt{052.alvinn}, the parallelized loop covers 97.5\% of the whole
application with the testing input, which results in 16.7$\times$
theoretical maximum speedup based on Almdahl's Law.

% dijkstra and swaptions

The \texttt{dijkstra} from MiBench and \texttt{swaptions} from PARSEC have
memory options like \texttt{malloc} and \texttt{mmap} inside the inner
loops, introducing significant variances in the runtime.


% Correlation and covariance
% Some reasoning here

% \begin{itemize}
% \item General Speed Plot: With different cores (1-28), all benchmarks
% \item Speedup Comparison: 28 cores performance compared with Privateer
% Runtime breakdown (how to minimize the overhead?)
% \end{itemize}

\begin{figure*}[ht]
  \includegraphics[width=\textwidth]{figures/multi-core-crop}
  \caption{Whole Application Speedup over Different Numbers of Cores}
  \label{fig:multi-core-scale}
\end{figure*}

\begin{figure*}[ht]
  \includegraphics[width=\textwidth]{figures/compare-privateer}
  \caption{Whole Application Speedup Comparison}
  \label{fig:speedup-compare}
\end{figure*}

\subsubsection{Effect of parallelization on vectorization}

instrumentation kills vectorization


\subsection{Static Analysis and Enablers}
Figures Needed:
\begin{itemize}
% (no need, just discuss the use of CAF in previous sections) \item CAF:
% with and without; no topping; with only BasicLoopAA and ScevAA

% (no need, show in the benchmark table) \item Coverage: Spec-DOALL
% percentage of coverage of each benchmark
\item Enablers: Enablers used for each benchmark
\item Optimization level: Performance with different optimization levels
\end{itemize}

Discussion Needed:

Present in a table, which enablers were used for all benchmarks

\subsection{Overhead Breakdown}

\begin{figure*}[htp]
  \includegraphics[width=0.9\textwidth]{figures/overheads}
\end{figure*}
\begin{figure*}[htp]
  \includegraphics[width=0.9\textwidth]{figures/comparison}
\end{figure*}
Huge speedup for dijkstra is largely due to the cheaper "malloc" that
we use in our heaps. Using a similar malloc in the sequential version
yields ~16x speedup in dijkstra and insignificant difference in the
other benchmarks.

\subsection{Misspeculation Analysis}
\begin{figure*}[htp]
  \includegraphics[width=0.9\textwidth]{figures/misspec}
  \caption{Misspeculation performance across speculative benchmarks}
  \label{fig:misspec}
\end{figure*}
Figure \ref{fig:misspec} shows how misspeculation affects the
performance of the six speculative benchmarks with misspeculation rates of
0.1\% and 0.2\%. Since none of the benchmarks exhibit misspeculation on the
given input sets, we artificially inject misspeculation at the end of
every 1000 and 500 iterations, respectively, to observe the performance
degradation. Note that misspeculation with \name is extremely unlikely, as
static analysis can disprove many speculative assumptions and for many of
the benchmarks, misspeculation is only possible during a heap check.
The inputs for \texttt{179.art} were not large enough to have
enough iterations for misspeculation at these rates, so we perform a
weighted average of non-misspeculating and misspeculating runs to achieve
an average corresponding to the respective rates. These results demonstrate
that \name performs well with only high confidence speculation.
\{\textbf{XXX} Is this why we can't parallelize the outer loop of
052.alvinn?\} \textbf{Something about how some benchmarks cannot misspec
with any input, only that analysis cannot prove that} A system that has
larger overheads has a larger chance of speculation (since they use more
speculative assumptions), but we minimize the amount of speculation.
Emphasize that misspeculation doesn't really happen in practice
% As the iteration within a checkpoint that misspeculation
% occurs can affect the recovery cost, we inject these misspeculations in the
% middle of a checkpoint chunk (\textbf{XXX} find better wording) to have a more
% realistic simulation, assuming that programs can misspeculate at any iteration with a
% uniform distribution.

\subsection{Power Consumption}

Power and energy stats here

